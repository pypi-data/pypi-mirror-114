{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba27a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch mlp for binary classification\n",
    "from numpy import vstack\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "import modeltrack.experiment as exp\n",
    "\n",
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(path, header=None)\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, :-1]\n",
    "        self.y = df.values[:, -1]\n",
    "        # ensure input data is floats\n",
    "        self.X = self.X.astype('float32')\n",
    "        # label encode target and ensure the values are floats\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    " \n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    " \n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    " \n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data(path, batch):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=batch, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=batch, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    "\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 10)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(10, 8)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(8, 1)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Sigmoid()\n",
    " \n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X\n",
    "    \n",
    "def train_one_epoch(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    One cycle of model training\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        # set previous gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute prediction for current batch\n",
    "        preds = model.forward(inputs)\n",
    "                \n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = loss_fn(preds, targets)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation to compute new gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update the model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        #compute the number of correct predictions\n",
    "        total_correct += (preds.round() == targets).type(torch.float).sum().item()\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_train_loss = train_loss / len(dataloader)\n",
    "    train_accuracy = total_correct / len(dataloader.dataset)\n",
    "\n",
    "    return avg_train_loss, train_accuracy\n",
    "\n",
    "\n",
    "def test_one_epoch(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "    One cycle of the model testing/validation cycle\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        with torch.no_grad():\n",
    "            preds = model.forward(inputs)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            test_loss += loss.item()\n",
    "            total_correct += (preds.round() == targets).type(torch.float).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(dataloader)\n",
    "    test_accuracy = total_correct / len(dataloader.dataset)\n",
    "\n",
    "    return avg_test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d09feaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a ModelTracker\n",
    "config = {\n",
    "    \"description\": \"This is my first neural net test run\",\n",
    "    \"seed\": 20,\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"max_epochs\": 10,\n",
    "    \"momentum\": 0.9,\n",
    "    \"overwrite\": True,\n",
    "}\n",
    "\n",
    "tracker = exp.ModelTracker(\"test-model\", config=config)\n",
    "\n",
    "# prepare the data\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
    "train_dl, test_dl = prepare_data(path, tracker.config.batch_size)\n",
    "\n",
    "# define the network\n",
    "model = MLP(34)\n",
    "loss_fn = BCELoss()\n",
    "optimizer = SGD(model.parameters(), lr=tracker.config.learning_rate, momentum=tracker.config.momentum)\n",
    "\n",
    "# inform tracker that training will commence\n",
    "tracker.start_training()\n",
    "\n",
    "# main training loop\n",
    "for epoch in range(tracker.config.max_epochs):\n",
    "    # train model\n",
    "    train_loss, train_acc  = train_one_epoch(train_dl, model, loss_fn, optimizer)\n",
    "\n",
    "    # evaluate model\n",
    "    test_loss, test_acc = test_one_epoch(test_dl, model, loss_fn)\n",
    "    \n",
    "    tracker.save_epoch_stats(train_loss, test_loss, train_acc, test_acc)\n",
    "    tracker.save_model(model, epoch, optimizer, test_loss)\n",
    "\n",
    "tracker.finish_training(model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
