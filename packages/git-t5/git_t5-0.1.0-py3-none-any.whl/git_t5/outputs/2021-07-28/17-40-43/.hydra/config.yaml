tokenizer:
  vocab_size: 30000
  special_tokens: []
  additional_special_tokens: []
  model_max_length: null
  show_progress: true
tokenizer_trainer:
  save_directory: ???
data:
  validation_size: 0.05
  max_sequence_length: null
  num_workers: null
  overwrite_cache: false
  dataset_name: null
  dataset_config_name: null
  dataset_path: null
  dataset_column: null
  model_path: ${model.model_path}
  tokenizer_path: ${model.tokenizer_path}
  use_fast_tokenizer: ${model.use_fast_tokenizer}
  cache_dir: ${model.cache_dir}
  mlm_probability: 0.15
  mean_noise_span_length: 3.0
model:
  max_epochs: ???
  train_batch_size: 8
  eval_batch_size: 8
  learning_rate: 5.0e-05
  weight_decay: 0.0
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-08
  adafactor: false
  scheduler_type: LINEAR
  warmup_steps: 0
  seed: 42
  model_path: null
  model_type: null
  config_name: null
  tokenizer_path: null
  cache_dir: null
  use_fast_tokenizer: false
  dtype: float32
