tokenizer:
  vocab_size: 30000
  special_tokens: []
  additional_special_tokens: []
  model_max_length: null
  show_progress: true
tokenizer_trainer:
  save_directory: ???
model:
  model_path: null
  model_type: null
  config_name: null
  tokenizer_path: null
  use_fast_tokenizer: false
  cache_dir: null
  max_epochs: ${trainer.max_epochs}
  train_batch_size: 8
  eval_batch_size: 8
  learning_rate: 5.0e-05
  weight_decay: 0.0
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-08
  adafactor: false
  scheduler_type: LINEAR
  warmup_steps: 0
  dtype: float32
  seed: 42
data:
  dataset_name: null
  dataset_config_name: null
  dataset_path: null
  dataset_column: null
  model_path: ${model.model_path}
  tokenizer_path: ${model.tokenizer_path}
  use_fast_tokenizer: ${model.use_fast_tokenizer}
  cache_dir: ${model.cache_dir}
  overwrite_cache: false
  validation_size: 0.05
  max_sequence_length: null
  num_workers: null
  mlm_probability: 0.15
  mean_noise_span_length: 3.0
trainer:
  output_dir: ???
  max_epochs: 3
  logging_steps: 500
  save_steps: 500
  eval_steps: 2000
  push_to_hub: false
  push_to_hub_model_id: null
  push_to_hub_organization: null
  push_to_hub_token: null
  checkpoint_dir: null
